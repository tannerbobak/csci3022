{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Homework 6: Bootstrapping, Hypothesis Testing, P-Hacking, and Simple Linear Regression \n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5 PM on Friday July 13**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**. \n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do Cell $\\rightarrow$ Run All as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3) | [Problem 4](#p4) | [Problem 5](#p5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "### [12 points] Problem 1 - Hypothesis Testing Whether your Co-worker is a Doofus\n",
    "\n",
    "You are working as a Data Scientist for an internet company. Your co-worker, Daley Jennanigans, is a lovable scamp! Unfortunately, Daley also makes a lot of mistakes throughout the day as the two of you team up to tackle some inference work regarding your company's customers. In each case, clearly explain why Daley's hypothesis testing setup or conclusion is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Daley has some data on the characteristics of customers that visited the company's website over the previous month.  He wants to perform an analysis on the proportion of last month's website visitors that bought something.  Let $X$ be the random variable describing the number of website visitors who bought something in the previous month, and suppose that the population proportion of visitors who bought something is $p$. Daley is particularly interested to see if the data suggests that more than 10% of website visitors actually buy something.  He decides to perform the test with a null hypothesis of $H_0: \\hat{p} = 0.10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Daley decides instead to do his hypothesis test with a null hypothesis of $H_0: p < 0.10$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally on track with reasonable hypotheses of $H_0: p = 0.10$ and $H_1: p > 0.10$, Daley computes a normalized test-statistic of $z = -1.4$ for the sample proportion and concludes that since $z = -1.4 < 0.05$ there is sufficient statistical evidence at the $\\alpha = 0.05$ (95%) significance level that the proportion of customers who buy something is less than 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Daley is again conducting the hypothesis test of $H_0: p = 0.10$ and $H_1: p > 0.10$. He computes a p-value of $0.03$, and thus concludes that there is only a 3% probability that the null hypothesis is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "### [20 points] Problem 2 - Evaluating Pizza Delivery Performance \n",
    "\n",
    "The manager of a pizza chain with multiple locations likes to keep meticulous data on his pizza deliveries.  The data from more than 1200 deliveries in May 2014 is stored in [`pizza.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7mvrrc671). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP = pd.read_csv(\"data/pizza.csv\")\n",
    "dfP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: The manager's goal is to have an average delivery time across all branches of less than 30 minutes.  Perform an appropriate hypothesis test at the $\\alpha=0.05$ significance level to evaluate whether this goal has been achieved.  Be sure to clearly state your null and alternate hypotheses, describe your testing procedure, and show all calculations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The pizza shop has fallen on hard times and is considering closing a branch. The manager knows that the East branch is the lowest performing branch, followed by the West branch, as measured by the the mean number of pizzas per delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for branch in set(dfP[\"branch\"]):\n",
    "    print(\"{:6s} branch mean pizzas/delivery = {:0.3f}\".format(branch, dfP.loc[dfP[\"branch\"]==branch, \"pizzas\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the manager conclude that the East branch is performing statistically significantly lower than the other two branches? Can he conclude that the West branch performs significantly worse than the Centre branch? Conduct hypothesis tests to answer these questions. Both of those questions will help him decide whether or not to close the East branch. The stakes are higher when making the decision to close an entire branch of pizza restaurant, so the manager decides to use the $\\alpha=0.01$ significance level for these tests. Be sure to clearly describe your hypotheses and methodology, and show any relevant computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: It has been hypothesized that two of the manager's finest drivers, Mario and Luigi, deliver different numbers of pizzas per delivery. Conduct an appropriate hypothesis test at the $\\alpha = 0.05$ significance level to determine if there is any statistically significant difference between the mean number of pizzas per delivery for the two drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "### [20 points] Problem 3 - Naps vs Coffee for Memory? \n",
    "\n",
    "It is estimated that [about 75% of adults](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4997286/) in the United States drink coffee. Often, coffee is used to replace the need for sleep. It works alright, or so I think. Let's find out, in this exciting homework problem!\n",
    "\n",
    "One recent study investigated the effects of drinking coffee, taking a nap, and having a [\"coffee-nap\"](https://lifehacker.com/naps-vs-coffee-which-is-better-when-youre-exhausted-1730643671) - the practice of drinking some coffee *and then* having a short nap. The study broke participants up into three groups of 10 participants each, where the groups would have a nap, or have a coffee, or have a coffee-nap, then perform a task where their reaction time was measured. In previous experiments the mean reaction time measurement was found to be normally distributed. The reaction time means (milliseconds, ms) and standard deviations for the three groups of participants are given in the table below.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\textrm{Group} & \\textrm{Sample Size} & \\textrm{Mean} & \\textrm{Standard Deviation} \\\\\n",
    "\\hline \n",
    "\\textrm{Coffee+Nap} & 10 & 451.3 & 31.9 \\\\ \n",
    "\\textrm{Coffee} & 10 & 494.2 & 39.6 \\\\ \n",
    "\\textrm{Nap} & 10 & 492.8 & 45.2 \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A**: Compute a 95% t-confidence interval for the mean reaction time measurement for participants in each of these three groups. (You should find three separate confidence intervals.) Do all computations in Python, and report the results.  Can you make any conclusions regarding whether coffee, naps or both (coffee-naps) are better for faster reaction times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use an appropriate hypothesis test to determine if there sufficient evidence, at the $\\alpha = 0.05$ significance level, to conclude that taking a nap promotes faster reaction time than drinking coffee.  Be sure to clearly explain the test that you're doing and state all hypotheses. Do all computations in Python, and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use an appropriate hypothesis test to determine if there is sufficient evidence, at the $\\alpha = 0.05$ significance level, to conclude that taking a coffee-nap promotes faster reaction time than only drinking coffee, or only having a nap.  Be sure to clearly explain the test that you're doing and state all hypotheses. Do all computations in Python, and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Compute a 95% confidence interval for the standard deviation of reaction time for coffee-nap takers. Do all computations in Python, and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p4'></a>\n",
    "\n",
    "### [15 points] Problem 4 - Bad Science for Fun and Profit \n",
    "\n",
    "[Data Dredging](https://en.wikipedia.org/wiki/Data_dredging) and [p-hacking](https://www.explainxkcd.com/wiki/index.php/882:_Significant) are umbrella terms for the dangerous practice of automatically testing a large number of hypotheses on the entirety or subsets of a single dataset in order to find statistically significant results. In this exercise we will focus on the idea of testing hypotheses on subsets of a single data set.  \n",
    "\n",
    "Johnny Nefarious has landed his first data science internship at an online marketing firm.  His primary summer project has been to design and test a new email advertisement for his company's best-selling product. To test his advertisement his supervisors have allowed him to send his ad to 4 targeted customer groups of 50 people every day for a month, and keep track of which email client (Outlook or Gmail) the customers use.\n",
    "\n",
    "The effectiveness of online advertising is typically measured by the ad's [click-through rate](https://en.wikipedia.org/wiki/Click-through_rate) (CTR), which is defined to be the _proportion_ of users that click on an advertisement. The company's standard email advertisement has a CTR of $0.05$.  Johnny is hoping to land a permanent position at the company when he graduates, so he's **really** motivated to show his supervisors that the CTR of his email advertisement is a (statistically) significant improvement over their previous ad. \n",
    "\n",
    "The data from Johnny's summer experiment can be found in [`email.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7qay9y674). Load this dataset into Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAd = pd.read_csv(\"data/email.csv\")\n",
    "dfAd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: State the null and alternate hypotheses that Johnny should test to see if his ad campaign is an improvement over the company's standard mailer with a CTR of $0.05$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Test the hypothesis from **Part A** at the $\\alpha = 0.05$ significance level using a p-value test. Is there sufficient evidence for Johnny to conclude that his ad campaign is an improvement?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: \"Aw, shucks!\", Johnny thinks. This is the part where he decides to resort to some questionable science.  Maybe there is a reasonable subset of the data for which his alternative hypothesis is supported?  Can he find it?  Can he come up for a reasonable justification for why this subset of the data should be considered while the rest should be discarded? \n",
    "\n",
    "Here are the **rules**: Johnny cannot modify the original data (e.g. by adding nonexistent clicks to certain groups) because his boss will surely notice.  Instead he needs to find a subset of the data for which his hypothesis is supported by a p-value test at the $\\alpha = 0.05$ significance level _and_ be able to explain to his supervisors why his sub-selection of the data is reasonable.  \n",
    "\n",
    "In addition to your explanation of why your successful subset of the data is potentially reasonable, be sure to thoroughly explain the details of the tests that you perform and show all of your Python computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p5'></a>\n",
    "\n",
    "### [15 points] Problem 5 - Simple Linear Regression for the Consumption of Ice Cream Treats\n",
    "\n",
    "The data in [`icecream.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n80849467a) contains information on a particular runner's ice cream eaten (in scoops) and miles run on particular days over the past year. In this exercise you will construct a simple linear regression model for the response variable \"amount of ice cream consumed\" (`scoops`), using \"number of miles run\" (`miles`) as the feature. Load the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTreats = pd.read_csv(\"data/icecream.csv\")\n",
    "dfTreats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Perform a simple linear regression with `miles` as the feature and `scoops` as the response.  Report the estimated regression model in the form $Y = \\alpha + \\beta x$. Do all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Make a scatter-plot of the data with `miles` as the feature and `scoops` as the response, and overlay the estimated regression line.  Clearly label all relevant plot elements and include a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Give a physical interpretation of the coefficients $\\hat{\\alpha}$ and $\\hat{\\beta}$, estimated from your model. Is the relationship between run lengths and ice cream consumption positive or negative? Fully justify your responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: How much ice cream does your simple linear regression model predict the runner will consume if they run a marathon (26.2 miles)? What are potential drawbacks to this model for ice cream consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part E**: Are there any other features you think should be added to the model, making this a *multiple* linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p6'></a>\n",
    "\n",
    "### [15 points] Problem 6 - \"Central Limit-ish Theorem\" for the Log-normal Distribution\n",
    "\n",
    "The log-normal distribution arises in many applications, such as (but not limited to!) modeling the distribution of extreme values like monthly rainfall, modeling the distribution of income for the bulk of the population, and the length of some board games. It is useful, but can be a little tough to wrap your head around at first. There is a \"logarithm\" right in the name, after all. So let's tame this beast.\n",
    "\n",
    "Execute the code below to obtain a sample of 10,000 random draws from a [log-normal distribution](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.lognormal.html) with parameters $\\mu = 3$ and $\\sigma = 0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.25\n",
    "mu = 3\n",
    "n = 10000\n",
    "x = pd.Series(np.random.lognormal(mean=mu, sigma=sigma, size=n))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "x.hist(ax=ax, edgecolor=\"white\", bins=20, normed=True)\n",
    "ax.set_xlim([0,50])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('X ~ LogNorm({}, {})'.format(mu,sigma))\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Recall that the log-normal pdf for a random variable $X$ is given by\n",
    "$$f(x \\mid \\mu, \\sigma) = \\dfrac{1}{x\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{\\log{x} - \\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "Thus, if you have a random variable $X \\sim LogNorm(\\mu, \\sigma)$, the transformed random variable $Y = \\log(X)$ must have $Y \\sim N(\\mu, \\sigma^2)$. Verify this by plotting a density histogram of $Y = \\log(X)$, along with the normal pdf $f(y \\mid \\mu, \\sigma^2)$. Be sure to label everything in your _single-panel_ figure and include a legend. Write 1-2 sentences commenting on the agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Now, read in the file [`lognorm.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7bg98m66x), available linked here and on Piazza. These are 25 samples from a log-normal distribution with unknown parameters $\\mu$ and $\\sigma$. Use bootstrapping with at least 1,000 re-sampling iterations to obtain 95% confidence intervals for each of the median and the parameter $\\mu$ of the unknown log-normal distribution from which these data originate. Note that the two are related by $\\tilde{x} = e^{\\mu}$, so the two confidence intervals are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: If $Y$ is normally distributed with mean $\\mu$ and standard deviation $\\sigma$, the Central Limit Theorem and what is commonly known as the [\"68-95-99.7 Rule\"](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) suggest that:\n",
    "* 68.3% of the data for $Y$ fall within the interval $[\\mu - \\sigma, \\mu + \\sigma]$\n",
    "* 95.4% of the data fall within the interval $[\\mu - 2\\sigma, \\mu + 2\\sigma]$\n",
    "* 99.7% of the data fall within the interval $[\\mu - 3\\sigma, \\mu + 3\\sigma]$\n",
    "\n",
    "If we have $X \\sim LogNorm(\\mu, \\sigma)$, then $Y = \\log(X) \\sim N(\\mu, \\sigma^2)$ follows this 68-95-99.7 Rule for normal distributions. Since $Y = \\log(X)$ is a one-to-one function, we can develop a similar 68-95-99.7 Rule for the log-normal distribution. This is just one of many reasons why we love one-to-one functions! Take the 68% part of the rule, for example.\n",
    "\n",
    "* 68.3% of the data for $Y$ are within $[\\mu - \\sigma, \\mu + \\sigma]$.\n",
    "* This means that 68.3% of the data for $X = e^Y$ are within $\\left[ e^{\\mu - \\sigma}, e^{\\mu + \\sigma}\\right] = \\left[ e^{\\mu}/e^{\\sigma}, ~~ e^{\\mu} e^{\\sigma}\\right]$\n",
    "* For brevity's sake, we often rewrite $m = e^{\\mu}$ and $s = e^{\\sigma}$, so that this interval becomes $[m/s, ms]$. Note that $m = \\tilde{x}$ is the median.\n",
    "\n",
    "Formulate a hypothesis regarding the form for the 95 and 99.7% parts of the Rule are for the log-normal distribution. Then, finish the code below to verify that your hypotheses are correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

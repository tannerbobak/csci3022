{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Homework 6: Bootstrapping, Hypothesis Testing, P-Hacking, and Simple Linear Regression \n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5 PM on Friday July 13**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**. \n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do Cell $\\rightarrow$ Run All as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3) | [Problem 4](#p4) | [Problem 5](#p5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "### [12 points] Problem 1 - Hypothesis Testing Whether your Co-worker is a Doofus\n",
    "\n",
    "You are working as a Data Scientist for an internet company. Your co-worker, Daley Jennanigans, is a lovable scamp! Unfortunately, Daley also makes a lot of mistakes throughout the day as the two of you team up to tackle some inference work regarding your company's customers. In each case, clearly explain why Daley's hypothesis testing setup or conclusion is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Daley has some data on the characteristics of customers that visited the company's website over the previous month.  He wants to perform an analysis on the proportion of last month's website visitors that bought something.  Let $X$ be the random variable describing the number of website visitors who bought something in the previous month, and suppose that the population proportion of visitors who bought something is $p$. Daley is particularly interested to see if the data suggests that more than 10% of website visitors actually buy something.  He decides to perform the test with a null hypothesis of $H_0: \\hat{p} = 0.10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Daley is wrong because he is interested in whether the population proportion $p$ is different from 0.10 not whether the sample proportion $\\hat{p}$ is different. He will know the sample proportion exactly but is interested in how that estimates $p$. Therefore, he should use $H_0 : \\, p = 0.10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Daley decides instead to do his hypothesis test with a null hypothesis of $H_0: p < 0.10$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daley is incorrect because the null hypothesis should be $p=0.10$ corresponding to the situation that 10% of website visitors buy something. This is the proper null hypothesis because a single value should be tested: the value in which there is no significant difference between the populations. In Daley's situation, he wants to see if there is a difference between a population in which 10% of visitors buy something and the real population. \n",
    "\n",
    "With the null hypothesis of $H_0 : \\, p = 0.10$, Daley can perform a proper 1-tailed P-Test with the alternative hypothesis that $H_1 : \\, p > 0.10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally on track with reasonable hypotheses of $H_0: p = 0.10$ and $H_1: p > 0.10$, Daley computes a normalized test-statistic of $z = -1.4$ for the sample proportion and concludes that since $z = -1.4 < 0.05$ there is sufficient statistical evidence at the $\\alpha = 0.05$ (95%) significance level that the proportion of customers who buy something is less than 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very incorrect because Daley is comparing a Z score to a p-value. Instead, he should compute the CDF of the normal distribution at $z = -1.4$ and subtract that from 1 because this is a one-tail p-test for the right tail of the distribution. This will obtain the correct p-value which can be compared to the significance level. \n",
    "\n",
    "Moreover, Daley is interpreting the meaing of the p-test incorrectly. If the p-value satisfies the comparison with the significance level with this alternative hypothesis, then we can simply say that we can reject the null hypothesis that $p=0.10$, not that the proportion of customers who buy something is less than 10%, $p < 0.10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Daley is again conducting the hypothesis test of $H_0: p = 0.10$ and $H_1: p > 0.10$. He computes a p-value of $0.03$, and thus concludes that there is only a 3% probability that the null hypothesis is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, Daley has it twisted. A p-value of 0.03 does not mean that there is a 3% chance that the null hypothesis is true, but rather that there is a 3% chance of observing something as extreme or more extreme than what was actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "### [20 points] Problem 2 - Evaluating Pizza Delivery Performance \n",
    "\n",
    "The manager of a pizza chain with multiple locations likes to keep meticulous data on his pizza deliveries.  The data from more than 1200 deliveries in May 2014 is stored in [`pizza.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7mvrrc671). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>operator</th>\n",
       "      <th>branch</th>\n",
       "      <th>driver</th>\n",
       "      <th>temperature</th>\n",
       "      <th>bill</th>\n",
       "      <th>pizzas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>1-May-14</td>\n",
       "      <td>33.708636</td>\n",
       "      <td>Laura</td>\n",
       "      <td>East</td>\n",
       "      <td>Bruno</td>\n",
       "      <td>71.433084</td>\n",
       "      <td>58.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>1-May-14</td>\n",
       "      <td>29.382070</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>East</td>\n",
       "      <td>Salvatore</td>\n",
       "      <td>64.952920</td>\n",
       "      <td>26.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>1-May-14</td>\n",
       "      <td>33.580664</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>West</td>\n",
       "      <td>Salvatore</td>\n",
       "      <td>49.113452</td>\n",
       "      <td>58.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>1-May-14</td>\n",
       "      <td>32.505369</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>East</td>\n",
       "      <td>Salvatore</td>\n",
       "      <td>64.872559</td>\n",
       "      <td>35.2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>1-May-14</td>\n",
       "      <td>25.493613</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>West</td>\n",
       "      <td>Salvatore</td>\n",
       "      <td>59.630052</td>\n",
       "      <td>38.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        day      date       time operator branch     driver  temperature  \\\n",
       "0  Thursday  1-May-14  33.708636    Laura   East      Bruno    71.433084   \n",
       "1  Thursday  1-May-14  29.382070  Melissa   East  Salvatore    64.952920   \n",
       "2  Thursday  1-May-14  33.580664  Melissa   West  Salvatore    49.113452   \n",
       "3  Thursday  1-May-14  32.505369  Melissa   East  Salvatore    64.872559   \n",
       "4  Thursday  1-May-14  25.493613  Melissa   West  Salvatore    59.630052   \n",
       "\n",
       "   bill  pizzas  \n",
       "0  58.4       4  \n",
       "1  26.4       3  \n",
       "2  58.1       3  \n",
       "3  35.2       3  \n",
       "4  38.4       2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfP = pd.read_csv(\"data/pizza.csv\")\n",
    "dfP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: The manager's goal is to have an average delivery time across all branches of less than 30 minutes.  Perform an appropriate hypothesis test at the $\\alpha=0.05$ significance level to evaluate whether this goal has been achieved.  Be sure to clearly state your null and alternate hypotheses, describe your testing procedure, and show all calculations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a sufficient number of samples to draw from (1,200) and there is reason to believe that the time data will be normally distributed, a one-tailed p-test is appropriate to examine the data. We can propose the null hypothesis $H_0 : \\, \\mu = 30$min per delivery and the alternative hypothesis that $H_1 : \\, \\mu < 30$ mins per delivery. Next, the mean ($\\bar{x}$), number of samples ($n$), and variance ($s^2$ which estimates $\\sigma^2$) of the samples from the data set must be calculated to obtain a Z-score which can be used to calculate the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean delivery time xbar: 29.455 min\n",
      "Variance in delivery time sigma2: 37.352 min^2\n",
      "Number of samples: 1266\n"
     ]
    }
   ],
   "source": [
    "xbar = np.mean(dfP[\"time\"])\n",
    "sigma2 = np.var(dfP[\"time\"], ddof=1)\n",
    "n = len(dfP[\"time\"])\n",
    "mu = 30\n",
    "\n",
    "print(\"Mean delivery time xbar: {:0.3f} min\".format(xbar))\n",
    "print(\"Variance in delivery time sigma2: {:0.3f} min^2\".format(sigma2))\n",
    "print(\"Number of samples: {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing $\\bar{x}=29.455$ min, $\\mu = 30$ min (from the null hypothesis), and $\\sigma^2 = 37.352$ min$^2$, these values can be plugged into the formula for calculating a Z-score:\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\bar{x} - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\n",
    "$$\n",
    "\n",
    "This is evaluated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -3.171\n"
     ]
    }
   ],
   "source": [
    "z = (xbar - mu)/np.sqrt(sigma2/n)\n",
    "\n",
    "print(\"Z = {:0.3f}\".format(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by calculating what the CDF of the normal distribution is up to $Z=-3.171$, we obtain the p-value for this data. This calculation is performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 0.00076\n"
     ]
    }
   ],
   "source": [
    "p = stats.norm.cdf(z)\n",
    "\n",
    "print(\"p-value = {:0.5f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p = 7.6 \\times 10^{-4} < 0.05 = \\alpha$, the null hypothesis that the mean delivery time is 30 mins can be rejected in favor of the alternative hypothesis that the mean delivery time is less than 30 mins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The pizza shop has fallen on hard times and is considering closing a branch. The manager knows that the East branch is the lowest performing branch, followed by the West branch, as measured by the the mean number of pizzas per delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centre branch mean pizzas/delivery = 3.368\n",
      "East   branch mean pizzas/delivery = 2.429\n",
      "West   branch mean pizzas/delivery = 3.207\n"
     ]
    }
   ],
   "source": [
    "for branch in set(dfP[\"branch\"]):\n",
    "    print(\"{:6s} branch mean pizzas/delivery = {:0.3f}\".format(branch, dfP.loc[dfP[\"branch\"]==branch, \"pizzas\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the manager conclude that the East branch is performing statistically significantly lower than the other two branches? Can he conclude that the West branch performs significantly worse than the Centre branch? Conduct hypothesis tests to answer these questions. Both of those questions will help him decide whether or not to close the East branch. The stakes are higher when making the decision to close an entire branch of pizza restaurant, so the manager decides to use the $\\alpha=0.01$ significance level for these tests. Be sure to clearly describe your hypotheses and methodology, and show any relevant computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if the East branch is performing worse than the other two, two hypothesis hypothesis tests are performed. First, the East and West branch are compared via a one-tailed p-test at the $\\alpha = 0.01$ significance level. Then, the East and Centre branch are compared via a one-tailed p-test at the $\\alpha = 0.01$ significance level. The null hypothesis for both tests is $H_0 : \\, \\mu_E = \\mu_X$ where $X$ is either the mean of the pizzas per delivery for East or Center branch ($E$ and $C$ respectively). The alternative hypothesis is then $H_1 : \\, \\mu_E < \\mu_X$. Next, the West branch is compared to the center branch using a one-tailed p-test with significance level $\\alpha = 0.01$ with the null hypothesis $H_0 : \\, \\mu_W = \\mu_C$ and the alternative hypothesis $H_1 : \\, \\mu_W < \\mu_C$. Calculations for all three of these tests are carried out similarly to Part A and are performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deliveries at each branch: W: 435, E: 410, C:421\n",
      "Mean deliveries at each branch: W: 3.207, E: 2.429, C:3.368\n",
      "Delivery variance at each branch: W: 2.091, E: 1.625, C:1.914\n",
      "z-score for test 1 = -8.304\n",
      "p-value for test 1 = 5.018e-17\n",
      "z-score for test 2 = -10.178\n",
      "p-value for test 2 = 1.237e-24\n",
      "z-score for test 3 = -1.668\n",
      "p-value for test 3 = 4.770e-02\n"
     ]
    }
   ],
   "source": [
    "west_side = dfP.loc[dfP[\"branch\"] == 'West', \"pizzas\"]\n",
    "east_side = dfP.loc[dfP[\"branch\"] == 'East', \"pizzas\"]\n",
    "centre_jabronies = dfP.loc[dfP[\"branch\"] == 'Centre', \"pizzas\"]\n",
    "\n",
    "# Verify that I can type\n",
    "print(\"Deliveries at each branch: W: {0}, E: {1}, C:{2}\".format(len(west_side), len(east_side), len(centre_jabronies)))\n",
    "\n",
    "# Basic information\n",
    "xw_bar = np.mean(west_side)\n",
    "xe_bar = np.mean(east_side)\n",
    "xc_bar = np.mean(centre_jabronies)\n",
    "\n",
    "print(\"Mean deliveries at each branch: W: {0:0.3f}, E: {1:0.3f}, C:{2:0.3f}\".format(xw_bar, xe_bar, xc_bar))\n",
    "\n",
    "varw = np.var(west_side, ddof=1)\n",
    "vare = np.var(east_side, ddof=1)\n",
    "varc = np.var(centre_jabronies, ddof=1)\n",
    "\n",
    "print(\"Delivery variance at each branch: W: {0:0.3f}, E: {1:0.3f}, C:{2:0.3f}\".format(varw, vare, varc))\n",
    "\n",
    "nw = len(west_side)\n",
    "ne = len(east_side)\n",
    "nc = len(centre_jabronies)\n",
    "\n",
    "# ------- TEST 1 -------\n",
    "z1 = (xe_bar-xw_bar)/np.sqrt(vare/ne + varw/nw)\n",
    "print(\"z-score for test 1 = {:0.3f}\".format(z1))\n",
    "p1 = stats.norm.cdf(z1)\n",
    "print(\"p-value for test 1 = {:0.3e}\".format(p1))\n",
    "\n",
    "# ------- TEST 2 -------\n",
    "z2 = (xe_bar-xc_bar)/np.sqrt(vare/ne + varc/nc)\n",
    "print(\"z-score for test 2 = {:0.3f}\".format(z2))\n",
    "p2 = stats.norm.cdf(z2)\n",
    "print(\"p-value for test 2 = {:0.3e}\".format(p2))\n",
    "\n",
    "# ------- TEST 3 -------\n",
    "z3 = (xw_bar-xc_bar)/np.sqrt(varw/nw + varc/nc)\n",
    "print(\"z-score for test 3 = {:0.3f}\".format(z3))\n",
    "p3 = stats.norm.cdf(z3)\n",
    "print(\"p-value for test 3 = {:0.3e}\".format(p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the tests above, we find the following results:\n",
    "\n",
    "Test 1:\n",
    "The p-value is about $5.0 \\times 10^{-17} < \\alpha$ so we reject the null hypothesis that the East branch delivers the same number of pizzas as the West branch at the 0.01 significance level.\n",
    "\n",
    "Test 2:\n",
    "The p-value is about $1.2 \\times 10^{-24} < \\alpha$ so we reject the null hypothesis that the East branch delivers the same number of pizzas as the Centre branch at the 0.01 significance level.\n",
    "\n",
    "Test 3:\n",
    "The p-value is about $0.048 > \\alpha$ so we fail to reject the null hypothesis that the West branch delivers the same number of pizzas as the Center branch at the 0.01 significance level. \n",
    "\n",
    "Based on these results, we can conclude that the East branch performs worse than the other two branches by a statistically significant margin when using the pizzas per delivery as a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: It has been hypothesized that two of the manager's finest drivers, Mario and Luigi, deliver different numbers of pizzas per delivery. Conduct an appropriate hypothesis test at the $\\alpha = 0.05$ significance level to determine if there is any statistically significant difference between the mean number of pizzas per delivery for the two drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, a two-tailed p-test with significance level $\\alpha = 0.05$ to compare Mario and Luigi. Calculations similar to the above will be carried out, but this time the null hypothesis will be $H_0 : \\, \\mu_M = \\mu_L$ and the alternative hypothesis will be $H_1 : \\, \\mu_M \\neq \\mu_L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for driver comparison = 0.09474\n"
     ]
    }
   ],
   "source": [
    "mario = dfP.loc[dfP[\"driver\"] == \"Mario\", \"pizzas\"]\n",
    "luigi = dfP.loc[dfP[\"driver\"] == \"Luigi\", \"pizzas\"]\n",
    "\n",
    "nm = len(mario)\n",
    "nl = len(luigi)\n",
    "\n",
    "xbar_mario = np.mean(mario)\n",
    "xbar_luigi = np.mean(luigi)\n",
    "\n",
    "varm = np.var(mario, ddof=1)\n",
    "varl = np.var(luigi, ddof=1)\n",
    "\n",
    "z = (xbar_mario - xbar_luigi)/np.sqrt(varm/nm + varl/nl)\n",
    "\n",
    "p = 2*stats.norm.cdf(-np.abs(z))\n",
    "print(\"p-value for driver comparison = {:0.5f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we find that $p=0.095 > 0.05 = \\alpha$. Therefore, we fail to reject the null hypothesis and conclude that Mario and Luigi deliver about the same number of pizzas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "### [20 points] Problem 3 - Naps vs Coffee for Memory? \n",
    "\n",
    "It is estimated that [about 75% of adults](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4997286/) in the United States drink coffee. Often, coffee is used to replace the need for sleep. It works alright, or so I think. Let's find out, in this exciting homework problem!\n",
    "\n",
    "One recent study investigated the effects of drinking coffee, taking a nap, and having a [\"coffee-nap\"](https://lifehacker.com/naps-vs-coffee-which-is-better-when-youre-exhausted-1730643671) - the practice of drinking some coffee *and then* having a short nap. The study broke participants up into three groups of 10 participants each, where the groups would have a nap, or have a coffee, or have a coffee-nap, then perform a task where their reaction time was measured. In previous experiments the mean reaction time measurement was found to be normally distributed. The reaction time means (milliseconds, ms) and standard deviations for the three groups of participants are given in the table below.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\textrm{Group} & \\textrm{Sample Size} & \\textrm{Mean} & \\textrm{Standard Deviation} \\\\\n",
    "\\hline \n",
    "\\textrm{Coffee+Nap} & 10 & 451.3 & 31.9 \\\\ \n",
    "\\textrm{Coffee} & 10 & 494.2 & 39.6 \\\\ \n",
    "\\textrm{Nap} & 10 & 492.8 & 45.2 \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A**: Compute a 95% t-confidence interval for the mean reaction time measurement for participants in each of these three groups. (You should find three separate confidence intervals.) Do all computations in Python, and report the results.  Can you make any conclusions regarding whether coffee, naps or both (coffee-naps) are better for faster reaction times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I borrow parts of the `ci_mean` function from Homework \\#5 and modify it to produce a confidence interval for a t-distribution as opposed to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ci_mean(xbar, sx, n, alp=0.05):\n",
    "    nu = n-1\n",
    "    t = stats.t.ppf(1-alp/2, nu)\n",
    "    scale = t*np.sqrt(sx/n)\n",
    "    return (xbar - scale, xbar + scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can easily find the confidence intervals by plugging in the given values from the table into the function using the default `alp=0.05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0% CI for coffee nap: [428.480,474.120] ms\n",
      "95.0% CI for coffee: [465.872,522.528] ms\n",
      "95.0% CI for nap: [460.466,525.134] ms\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAFNCAYAAAAZ7h6UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46482ef470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alp = 0.05\n",
    "\n",
    "# Coffee Nap\n",
    "ci1 = ci_mean(451.3, 31.9**2, 10)\n",
    "print(\"{0:0.1f}% CI for coffee nap: [{1:0.3f},{2:0.3f}] ms\".format(100*(1-alp), ci1[0], ci1[1]))\n",
    "\n",
    "# Coffee\n",
    "ci2 = ci_mean(494.2, 39.6**2, 10)\n",
    "print(\"{0:0.1f}% CI for coffee: [{1:0.3f},{2:0.3f}] ms\".format(100*(1-alp), ci2[0], ci2[1]))\n",
    "\n",
    "# Nap\n",
    "ci3 = ci_mean(492.8, 45.2**2, 10)\n",
    "print(\"{0:0.1f}% CI for nap: [{1:0.3f},{2:0.3f}] ms\".format(100*(1-alp), ci3[0], ci3[1]))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(ci1, [1,1], 'r', label=\"Coffee Nap\")\n",
    "plt.plot(ci2, [2,2], 'g', label=\"Coffee\")\n",
    "plt.plot(ci3, [3,3], 'b', label=\"Nap\")\n",
    "plt.legend()\n",
    "plt.yticks([],[])\n",
    "plt.xlabel(\"Reaction Time (ms)\")\n",
    "plt.title(\"Reaction Time CI Overlap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above graph, all three 95% confidence intervals overlap, so no conclusion can immediately be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use an appropriate hypothesis test to determine if there sufficient evidence, at the $\\alpha = 0.05$ significance level, to conclude that taking a nap promotes faster reaction time than drinking coffee.  Be sure to clearly explain the test that you're doing and state all hypotheses. Do all computations in Python, and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided here is sufficient to perform a one-tailed t-test at the $\\alpha = 0.05$ significance level that compares taking a nap to drinking coffee. Here the null hypothesis is $H_0 : \\, \\mu_N = \\mu_C$, that there is no difference in reaction time from drinking coffee or taking a nap. The alternative hypothesis is $H_1 : \\, \\mu_N < \\mu_C$, that taking a nap results in a faster reaction time than having coffee. Calculations here are very similar to the other problems in this homework, except a t-distribution is used in lieu of the normal distribution. These calculations are carried out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for Nap vs. Coffee: 0.471\n"
     ]
    }
   ],
   "source": [
    "xbar_n = 492.8\n",
    "xbar_c = 494.2\n",
    "\n",
    "var_n = 45.2**2\n",
    "var_c = 39.6**2\n",
    "\n",
    "n = 10 # samples the same for both\n",
    "nu = n - 1\n",
    "\n",
    "t = (xbar_n - xbar_c)/np.sqrt(var_n/n + var_c/n)\n",
    "p = stats.t.cdf(t, nu)\n",
    "print(\"p-value for Nap vs. Coffee: {:0.3f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $p = 0.471 > 0.05 = \\alpha$ we fail to reject the null hypothesis and conclude that having coffee or taking a nap result in about the same reaction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use an appropriate hypothesis test to determine if there is sufficient evidence, at the $\\alpha = 0.05$ significance level, to conclude that taking a coffee-nap promotes faster reaction time than only drinking coffee, or only having a nap.  Be sure to clearly explain the test that you're doing and state all hypotheses. Do all computations in Python, and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Compute a 95% confidence interval for the standard deviation of reaction time for coffee-nap takers. Do all computations in Python, and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p4'></a>\n",
    "\n",
    "### [15 points] Problem 4 - Bad Science for Fun and Profit \n",
    "\n",
    "[Data Dredging](https://en.wikipedia.org/wiki/Data_dredging) and [p-hacking](https://www.explainxkcd.com/wiki/index.php/882:_Significant) are umbrella terms for the dangerous practice of automatically testing a large number of hypotheses on the entirety or subsets of a single dataset in order to find statistically significant results. In this exercise we will focus on the idea of testing hypotheses on subsets of a single data set.  \n",
    "\n",
    "Johnny Nefarious has landed his first data science internship at an online marketing firm.  His primary summer project has been to design and test a new email advertisement for his company's best-selling product. To test his advertisement his supervisors have allowed him to send his ad to 4 targeted customer groups of 50 people every day for a month, and keep track of which email client (Outlook or Gmail) the customers use.\n",
    "\n",
    "The effectiveness of online advertising is typically measured by the ad's [click-through rate](https://en.wikipedia.org/wiki/Click-through_rate) (CTR), which is defined to be the _proportion_ of users that click on an advertisement. The company's standard email advertisement has a CTR of $0.05$.  Johnny is hoping to land a permanent position at the company when he graduates, so he's **really** motivated to show his supervisors that the CTR of his email advertisement is a (statistically) significant improvement over their previous ad. \n",
    "\n",
    "The data from Johnny's summer experiment can be found in [`email.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7qay9y674). Load this dataset into Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAd = pd.read_csv(\"data/email.csv\")\n",
    "dfAd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: State the null and alternate hypotheses that Johnny should test to see if his ad campaign is an improvement over the company's standard mailer with a CTR of $0.05$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Test the hypothesis from **Part A** at the $\\alpha = 0.05$ significance level using a p-value test. Is there sufficient evidence for Johnny to conclude that his ad campaign is an improvement?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: \"Aw, shucks!\", Johnny thinks. This is the part where he decides to resort to some questionable science.  Maybe there is a reasonable subset of the data for which his alternative hypothesis is supported?  Can he find it?  Can he come up for a reasonable justification for why this subset of the data should be considered while the rest should be discarded? \n",
    "\n",
    "Here are the **rules**: Johnny cannot modify the original data (e.g. by adding nonexistent clicks to certain groups) because his boss will surely notice.  Instead he needs to find a subset of the data for which his hypothesis is supported by a p-value test at the $\\alpha = 0.05$ significance level _and_ be able to explain to his supervisors why his sub-selection of the data is reasonable.  \n",
    "\n",
    "In addition to your explanation of why your successful subset of the data is potentially reasonable, be sure to thoroughly explain the details of the tests that you perform and show all of your Python computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p5'></a>\n",
    "\n",
    "### [15 points] Problem 5 - Simple Linear Regression for the Consumption of Ice Cream Treats\n",
    "\n",
    "The data in [`icecream.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n80849467a) contains information on a particular runner's ice cream eaten (in scoops) and miles run on particular days over the past year. In this exercise you will construct a simple linear regression model for the response variable \"amount of ice cream consumed\" (`scoops`), using \"number of miles run\" (`miles`) as the feature. Load the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTreats = pd.read_csv(\"data/icecream.csv\")\n",
    "dfTreats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Perform a simple linear regression with `miles` as the feature and `scoops` as the response.  Report the estimated regression model in the form $Y = \\alpha + \\beta x$. Do all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Make a scatter-plot of the data with `miles` as the feature and `scoops` as the response, and overlay the estimated regression line.  Clearly label all relevant plot elements and include a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Give a physical interpretation of the coefficients $\\hat{\\alpha}$ and $\\hat{\\beta}$, estimated from your model. Is the relationship between run lengths and ice cream consumption positive or negative? Fully justify your responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: How much ice cream does your simple linear regression model predict the runner will consume if they run a marathon (26.2 miles)? What are potential drawbacks to this model for ice cream consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part E**: Are there any other features you think should be added to the model, making this a *multiple* linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p6'></a>\n",
    "\n",
    "### [15 points] Problem 6 - \"Central Limit-ish Theorem\" for the Log-normal Distribution\n",
    "\n",
    "The log-normal distribution arises in many applications, such as (but not limited to!) modeling the distribution of extreme values like monthly rainfall, modeling the distribution of income for the bulk of the population, and the length of some board games. It is useful, but can be a little tough to wrap your head around at first. There is a \"logarithm\" right in the name, after all. So let's tame this beast.\n",
    "\n",
    "Execute the code below to obtain a sample of 10,000 random draws from a [log-normal distribution](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.lognormal.html) with parameters $\\mu = 3$ and $\\sigma = 0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.25\n",
    "mu = 3\n",
    "n = 10000\n",
    "x = pd.Series(np.random.lognormal(mean=mu, sigma=sigma, size=n))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "x.hist(ax=ax, edgecolor=\"white\", bins=20, normed=True)\n",
    "ax.set_xlim([0,50])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('X ~ LogNorm({}, {})'.format(mu,sigma))\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Recall that the log-normal pdf for a random variable $X$ is given by\n",
    "$$f(x \\mid \\mu, \\sigma) = \\dfrac{1}{x\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{\\log{x} - \\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "Thus, if you have a random variable $X \\sim LogNorm(\\mu, \\sigma)$, the transformed random variable $Y = \\log(X)$ must have $Y \\sim N(\\mu, \\sigma^2)$. Verify this by plotting a density histogram of $Y = \\log(X)$, along with the normal pdf $f(y \\mid \\mu, \\sigma^2)$. Be sure to label everything in your _single-panel_ figure and include a legend. Write 1-2 sentences commenting on the agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Now, read in the file [`lognorm.csv`](https://piazza.com/class_profile/get_resource/jhaqogsdelf76h/jja0n7bg98m66x), available linked here and on Piazza. These are 25 samples from a log-normal distribution with unknown parameters $\\mu$ and $\\sigma$. Use bootstrapping with at least 1,000 re-sampling iterations to obtain 95% confidence intervals for each of the median and the parameter $\\mu$ of the unknown log-normal distribution from which these data originate. Note that the two are related by $\\tilde{x} = e^{\\mu}$, so the two confidence intervals are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: If $Y$ is normally distributed with mean $\\mu$ and standard deviation $\\sigma$, the Central Limit Theorem and what is commonly known as the [\"68-95-99.7 Rule\"](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) suggest that:\n",
    "* 68.3% of the data for $Y$ fall within the interval $[\\mu - \\sigma, \\mu + \\sigma]$\n",
    "* 95.4% of the data fall within the interval $[\\mu - 2\\sigma, \\mu + 2\\sigma]$\n",
    "* 99.7% of the data fall within the interval $[\\mu - 3\\sigma, \\mu + 3\\sigma]$\n",
    "\n",
    "If we have $X \\sim LogNorm(\\mu, \\sigma)$, then $Y = \\log(X) \\sim N(\\mu, \\sigma^2)$ follows this 68-95-99.7 Rule for normal distributions. Since $Y = \\log(X)$ is a one-to-one function, we can develop a similar 68-95-99.7 Rule for the log-normal distribution. This is just one of many reasons why we love one-to-one functions! Take the 68% part of the rule, for example.\n",
    "\n",
    "* 68.3% of the data for $Y$ are within $[\\mu - \\sigma, \\mu + \\sigma]$.\n",
    "* This means that 68.3% of the data for $X = e^Y$ are within $\\left[ e^{\\mu - \\sigma}, e^{\\mu + \\sigma}\\right] = \\left[ e^{\\mu}/e^{\\sigma}, ~~ e^{\\mu} e^{\\sigma}\\right]$\n",
    "* For brevity's sake, we often rewrite $m = e^{\\mu}$ and $s = e^{\\sigma}$, so that this interval becomes $[m/s, ms]$. Note that $m = \\tilde{x}$ is the median.\n",
    "\n",
    "Formulate a hypothesis regarding the form for the 95 and 99.7% parts of the Rule are for the log-normal distribution. Then, finish the code below to verify that your hypotheses are correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
